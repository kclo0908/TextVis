<div align= "center">
    <h1> CSE 5525 - Final Project </h1>
    <h4> Kai Zhang (zhang.13253), Zhongwei Wan (wan.512), Kuan Chieh Lo (lo.311)</h4>
    <h4>First Time NLP</h4>
</div>

## Introduction

Large Language Models (LLMs) are trained on vast amounts of text data and have demonstrated remarkable capabilities in understanding and generating natural language. However, many LLMs have not been explicitly trained on visual data. Despite this, LLMs may have latent knowledge about visual objects through textual descriptions. For instance, a model trained only on text may have never "seen" a tree, yet it could have processed numerous descriptions detailing a tree's appearance.

Understanding whether LLMs can learn and reason about visual objects through language is interesting. It can test whether LLMs can ground language to visual objects. Also, if LLMs can conceptualize visual objects based solely on text, it would expand their application in multimodal tasks without necessitating additional training on expensive visual data. An example of this would be asking an LLM to describe the shape of a common object, such as a circle, and assessing whether the model correctly identifies attributes commonly associated with circles, such as "round" or "smooth".
